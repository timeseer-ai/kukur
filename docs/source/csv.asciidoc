// SPDX-FileCopyrightText: 2021 Timeseer.AI
//
// SPDX-License-Identifier: Apache-2.0
== CSV

Sources with `type = "csv"` configure CSV sources.

```toml
[source.<name>]
type = "csv"
path = "<path>"
metadata = "<path>"
metadata_fields = ["<name>", "<name>"]
metadata_mapping = "<name>"
metadata_value_mapping ="<name>"
format = "row|pivot|dir"
dictionary_dir = "<path>"
quality_mapping = "<name>"
file_encoding = "<codec>"
```

`path` is required for time series sources.
It is optional when used as a metadata source.

`file_encoding` is optional and defaults to `UTF-8`.
A list of all supported codecs can be found https://docs.python.org/3/library/codecs.html#standard-encodings[here].

Three CSV data models are supported:
- row based (series name, timestamp, value, quality (optional))
- pivot (multiple series with values at the same timestamp)
- directory based, one CSV file per tag

=== Row Based Format

A row based CSV data file does not contain a header row.
At least 3 columns are present:

- series name
- timestamp in RFC3339 format (up to nanosecond precision)
- numerical value (up to double precision floating point)

Example:

```csv
test-tag-1,2020-01-01T00:00:00Z,1
test-tag-1,2020-02-01T00:00:00Z,2
test-tag-2,2020-01-01T00:00:00Z,Product A
test-tag-2,2020-02-01T00:00:00Z,Product B
```

Alternatively, the third column can contain string values.
It is not possible to mix numerical and string values in one column.
This will cause all numerical values to be interpreted as strings.

Dictionary data is integer numerical data.
Labels are only for presenting to users.

=== Pivot Format

The header row of CSV data files in pivot format defines which time series are available.

Other rows start with a timestamp in RFC3339 format and contain one value for each series.

```csv
timestamp,test-tag-1,test-tag-2
2020-01-01T00:00:00Z,1,10
2020-02-01T00:00:00Z,2,11
```

=== Directory Based Format

The directory based format expects one CSV file per tag.
CSV files are formatted in the row based format, but without the series name.
They are named `<series name>.csv`.

`path` refers to the directory that contains the CSV files.

Example `test-tag-1.csv`:

```csv
2020-01-01T00:00:00Z,1
2020-02-01T00:00:00Z,2
```

=== Metadata

Metadata is configured in a matrix format.
A header row describes the metadata entry.

Supported types of metadata are:

- `series name` (required)
include::{include-path}/metadata-fields.asciidoc[]

Not all columns need to be present.

Example:

```csv
series name,unit,functional lower limit,functional upper limit,accuracy
test-tag-1,m,0,1,0.1
```

Extra columns will be ignored,
unless the `metadata_columns` parameter is present.
In that case all fields defined there - and only these - will be included,
including custom metadata fields.

Example:

```
series name,description,unit,process type,location
test-tag-1,"custom fields example",m,batch,Antwerp
```

```toml
[source.<name>]
metadata_colums = ["unit", "process type"]
```

Only the `unit` and the `process type` fields will be available in the resulting `Metadata`.

When the `dictionary name` field is present, the directory given in `dictionary_dir`
is searched for a file `<dictionary name>.csv`.
This file contains a comma separated mapping of numerical values to labels.

Example:

```csv
0,OFF
1,ON
```

Columns in a metadata CSV often do not match the names of metadata fields in Kukur.
An optional `metadata_mapping` maps Kukur field names to column names.

Example:

```toml
[source.<name>]
metadata_mapping = "ip21"

[metadata_mapping.ip21]
"series name" = "NAME"
description = "IP_DESCRIPTION"
unit = "IP_ENG_UNITS"
```

Where the metadata CSV contains:

```csv
NAME,IP_ENG_UNITS,lower limit
test-tag-1,kg,1
```

Fields that are not included in the mapping,
such as `functional lower limit` in the example,
translate to the corresponding metadata field or are skipped altogether.

Metadata mappings can be shared between sources.

Values in a metadata CSV can also be different.
The optional `metadata_value_mapping` maps Kukur metadata field values to values as they appear in a source.

Example:

```toml
[source.<name>]
metadata_value_mapping = "example_value_mapping"

[metadata_value_mapping.example_value_mapping."interpolation type"]
LINEAR = "linear"
STEPPED = "stepped"

[metadata_value_mapping.example_value_mapping."data type"]
FLOAT64 = ["int16", "int32"]
```

In this example,
when the `interpolation type` column contains the value `linear`,
Kukur will interpret it as the expected uppercase `LINEAR`.
When the `data type` column contains either `int16` or `int32`,
Kukur will interpret it as `FLOAT64`.

```csv
series name,interpolation type,data type
test-tag-1,linear,int32
```

`metadata_mapping` and `metadata_value_mapping` can be used together
to map wildly different metadata formats to a CSV supported by Kukur.

=== Quality

There is a possibility to add a quality column in the CSV file.
Check the
ifdef::sources[]
<<Quality, source documentation>>
endif::sources[]
ifndef::sources[]
link:sources.asciidoc#Quality[source documentation]
endif::sources[]
to configure the mapping of a value in the quality column to a quality status known to Kukur.

A quality column is not available for a CSV file with a pivot data format.

=== Azure Blob Storage

Kukur can load CSV files from Azure Blob Storage.
This requires the https://pypi.org/project/azure-storage-blob/[azure-storage-blob] and https://pypi.org/project/azure-identity/[azure-identity] Python packages.

The following

[source,toml]
----
[source."My Azure Source"]
...
loader = "azure-blob"
azure_connection_string = "DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<storage account name>"
azure_container = "<container name>"
azure_identity = "default"
----

Paths provided to `path`, `metadata` or `dictionary_dir` will be relative to the container root.

The `azure_identity` field is optional.
The special value `default` causes connections to be made using the https://docs.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python[default Azure credentials].
This is the only supported value and allows connections using a managed service identity.

When the `azure_identity` field is omitted,
the `azure_connection_string` needs to contain the necessary secrets (SAS token, Access key).
