== Databricks SQL Warehouse

Connections to a https://www.databricks.com/product/databricks-sql[Databricks SQL warehouse] use the https://docs.databricks.com/api/workspace/statementexecution[Statement Execution] REST API.
Alternatively the https://docs.databricks.com/en/integrations/odbc/index.html[Databricks ODBC Driver] can be used.
The driver needs to be installed separately.

Databricks recommends using serverless SQL warehouses when available.

=== Statement Execution

The Databricks Statement Execution API for SQL Warehouses returns data efficiently using Apache Arrow.

Two queries are supported.
Metadata for time series can be queried by defining a `list_query`.
Data for time series is queries using the `data_query`.

```toml
[source.<name>]
type = "databricks-sql"
provider = "REST"
tag_columns = ["series name"]
field_columns = ["value"]
metadata_columns = [] # optional: the names of the columns that return metadata
list_query = "" # optional: the query used to list time series
list_columns = [""] # optional: the names of the columns returned by the list query
data_query = "" # optional: the query used to request data for a time series
metadata_value_mapping = "" # optional: map metadata values
quality_mapping = "" # optional: map a quality column

[source.<name>.connection]
host = "" # required: the host name of the SQL Warehouse Server
warehouse_id = "" # required: the ID of the SQL Warehouse
password = "" # required: the Personal Access Token (PAT)
proxy = "" # optional: the HTTPS proxy server to use
verify_ssl = True # optional: set to `False` to disable SSL verification
timeout_seconds = 60 # optional: The maximum wait time for a request to complete
```

Supported authentication methods are:

* Databricks personal access token

The `data_query` supports named parameters only.
Each tag defined in `tag_columns` is provided as a named parameter,
with spaces replaced by underscores.
The time range is provided by the `start_date` and `end_date` named parameters of type `TIMESTAMP`.

For example:

```
[source.Databricks]
type = "databricks-sql"
list_query = """
select distinct `series name` from tsdeltalake.poc.tsai_antwerp
"""
list_columns = ["series name"]
data_query = """
select ts, value
from tsdeltalake.poc.tsai_antwerp
where `series name` = :series_name
  and ts >= :start_date
  and ts < :end_date

[source.Databricks.connection]
host = "adb-5136731089164599.19.azuredatabricks.net"
http_path = "/sql/1.0/warehouses/2cf2d4cd375bb81a"
url = "https://adb-5136731089164599.19.azuredatabricks.net"
warehouse_id = "2cf2d4cd375bb81a"
password = ""
"""
```

=== Databricks ODBC Driver

All connection options supported in the
ifdef::sources[]
<<ODBC, ODBC source>>
endif::sources[]
ifndef::sources[]
link:odbc.asciidoc[ODBC source]
endif::sources[]
are supported by the `databricks-sql` source.
The `odbc` source could be used instead,
but the `databricks-sql` source provides more convenience by pre-configuring various connection settings.

Authentication methods supported out-of-the-box are:

* Databricks personal access token
* OAuth machine-to-machine authentication
* Azure managed identities authentication

Other authentication methods are supported by configuring a `connection_string` with the required https://learn.microsoft.com/en-us/azure/databricks/integrations/odbc/authentication[authentication options].

```toml
[source.Databricks]
type = "databricks-sql"
provider = "ODBC"
...

[source.Databricks.connection]
driver = "/opt/simba/spark/lib/64/libsparkodbc_sb64.so"
host = ""
port = 443
http_path = ""
Azure_workspace_resource_id = "" # Optional, only required for authentication using managed identities.
oauth_client_id = "" # Optional, only required for OAuth authentication.
oauth_secret = "" # Optional, only required for OAuth authentication.
password = "" # Optional, only required for personal access token authentication.
```

The example configuration below connects the `tsaideltalake.poc.tsai_antwerp` table using OAuth M2M authentication.

First follow the steps in the https://learn.microsoft.com/en-us/azure/databricks/integrations/odbc/authentication#authentication-m2m[OAuth machine-to-machine authentication documentation].

Then,
open the SQL Warehouse in the Databricks Workspace.
Go to `Connection details`.

Copy the `Server hostname` and the `HTTP path`.

```toml
[source.Databricks]
type = "databricks-sql"
provider = "ODBC"
list_query = """
select distinct `series name` from tsdeltalake.poc.tsai_antwerp
"""
list_columns = ["series name"]
data_query = """
select ts, value
from tsdeltalake.poc.tsai_antwerp
where `series name` = ?
  and ts >= ?
  and ts < ?
"""

[source.Databricks.connection]
host = "adb-5136731089164599.19.azuredatabricks.net"
http_path = "/sql/1.0/warehouses/2cf2d4cd375bb81a"
oauth_client_id = "5edbb8d4-9ec4-4313-99bb-28a18982f339"
oauth_secret = "verysecuresecret"
```

==== Connecting to Databricks SQL Warehouses on Azure using Managed Identities

On Azure,
using Managed Identities is the preferred authentication method since it avoids having to use and manage secrets.

The Managed Identity of the VM needs to be defined as a Service Principal in Databricks.

The full documentation to achieve this is available at https://learn.microsoft.com/en-us/azure/databricks/dev-tools/azure-mi-auth[Set up and use Azure managed identities authentication for Azure Databricks automation]

The required steps are:

- Go to the VM in the Azure portal. Copy the `Object ID` of the VM Identity.
- Go to Entra ID. Search for the `Object ID` copied in the previous step. Copy the `Application ID`.
- Go to the https://accounts.azuredatabricks.net/[Databricks Account console].
- In `User management` - `Service principals`, choose `Add service principal`.
- Choose `Microsoft Entra ID managed`. Paste the `Application ID`. Give the service principal a name.
- Go to `SQL Warehouses` in the Databricks Workspace. Add `Can use` permissions to the service principal.
- In the `Catalog`, grant `Data Reader` permissions to the service principal.

```toml
[source.Databricks.connection]
host = "adb-5136731089164599.19.azuredatabricks.net"
http_path = "/sql/1.0/warehouses/2cf2d4cd375bb81a"
azure_workspace_resource_id = "/subscriptions/4eaf5c02-76be-4f45-b92a-d5882e686c95/resourceGroups/rg-kukur/providers/Microsoft.Databricks/workspaces/kukur-demo"
```

==== Connecting to Databricks SQL Warehouses using a Personal Access Token

Generate a personal access token in 'Settings' - 'Developer' - 'Access tokens'.
This token will have your permissions.
Do not use this except for testing or local work.

```toml
[source.Databricks.connection]
host = "adb-5136731089164599.19.azuredatabricks.net"
http_path = "/sql/1.0/warehouses/2cf2d4cd375bb81a"
password = "verysecuresecret"
```

==== Using Alternative Authentication Options

Refer to https://learn.microsoft.com/en-us/azure/databricks/integrations/odbc/authentication[Authentication settings for the Databricks ODBC Driver] for alternative authentication options.

The `connection_string` can be passed as one string:

```toml
[source.Databricks]
type = "databricks-sql"
provider = "ODBC"
connection_string = """
Driver=/opt/simba/spark/lib/64/libsparkodbc_sb64.so;
Host=adb-5136731089164599.19.azuredatabricks.net;
Port=443;
HTTPPath=/sql/1.0/warehouses/2cf2d4cd375bb81a;
SSL=1;
ThriftTransport=2;
AuthMech=11;
Auth_Flow=3;
Azure_workspace_resource_id=/subscriptions/4eaf5c02-76be-4f45-b92a-d5882e686c95/resourceGroups/rg-kukur/providers/Microsoft.Databricks/workspaces/kukur-demo;
"""
```

==== Connecting using a DSN on Windows

After installing the ODBC driver,
open `ODBC Data Sources (64-bit)`.

Create a new System DSN.

* Configure the host and port.
* Select the required authentication mechanism (`OAuth2` or `User Name and Password`). Use `token` as the `User Name` for Personal Access Token authentication.
* Use HTTP as the `Thrift Transport`.
* In `HTTP Options...` configure the `HTTP Path`.
* `Enable SSL` in the `SSL Options...`

Then,
refer to the DSN name in `source.connection_string`.

```toml
[source.Databricks]
type = "databricks-sql"
provider = "ODBC"
connection_string = """
DSN=Databricks;
"""
```
