// SPDX-FileCopyrightText: 2022 Timeseer.AI
// SPDX-License-Identifier: Apache-2.0

== Delta Lake

Sources with `type = "delta"` configure https://delta.io/[Delta Lake] sources.

Delta Lake supports the `row` and `pivot` formats:

```
[source.<name>]
type = "delta"
format = "row|pivot"
uri = "<uri of Delta Lake>"
tag_columns = ["series name"]
field_columns = ["value"]
sort_by_timestamp = false

[[source.<name>.partitions]]
origin = "tag"
key = "series name"

[[source.<name>.partitions]]
origin = "timestamp"
key = "YEAR"
format = "%Y"
column = "year"

```

`format` defaults to `"row"`.

`uri` is required.

`sort_by_timestamp` is an optional parameter to sort unordered delta lake sources. 
Default is `False`.

Metadata in Delta Lakes is not supported.
Use a different `metadata_type` to connect to metadata.

For example:

```toml
metadata_type = "csv"
```

=== Connecting to a Delta Lake on Azure

The https://github.com/delta-io/delta-rs[delta-rs] library used by Kukur supports Delta Lakes in an Azure storage account.
Unfortunately it uses environment variables for configuration.
This means one instance of Kukur is able to connect to one storage account only.

For example:

```toml
[source."Delta"]
type = "delta"
uri = "abfss://poc@satsdeltalake.dfs.core.windows.net/tsai-antwerp"
```

This configuration connects to the `satsdeltalake` storage account.
It opens the `tsai-antwerp` Delta Lake in the `poc` container.

Multiple https://github.com/delta-io/delta-rs/blob/python-v0.5.5/rust/src/storage/azure/mod.rs[authentication options] are available.

The recommended authentication method while running on Azure includes using a Managed Identity.
To do so,
the `AZURE_STORAGE_ACCOUNT` environment variable should duplicate the storage account given in the `uri`.
Next to this,
when not running on Azure App Service,
the `IDENTITY_HEADER` environment variable should be set to any value,
for example `foo`.

=== Column mapping

Accepted by Delta Lake sources that use the row format.
This configuration allows mapping the columns of the Delta Lake to the
columns expected by Kukur.
This is done by setting the `column_mapping` option for the source.

```toml
[source.<name>.column_mapping]
"series name" = "name"
"ts" = "timestamp"
"value" = "value"
"quality" = "quality column"
```

`quality` mapping is optional.

=== Partitioning

For the `row` format,
it is possible to take advantage of delta lake partitioning by defining one or more partitions:

```toml
[[source.<name>.partitions]]
origin = "tag"
key = "location"
```

Every tag based partition key must be included in the `tag_columns` option.

Timestamp based partitioning is also supported:

```toml
[[source.<name>.partitions]]
origin = "timestamp"
key = "YEAR"
format = "%Y"
column = "year"
```

For timestamp based partitions:

- `key` indicates the resolution to be used when querying for partitions.
Supported values are `YEAR`, `MONTH` and `DAY`.

- `format` indicates how the timestamp is used for partitioning. Defaults to `%Y`, `%Y-%m`, `%Y-%m-%d` repectively.
It accepts the https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior[formatting options] supported by Python.

- `column` is optional.
Needed if the name of the column used for partitioning does not exactly match the `key` value.

=== Custom datetime format

The `data_datetime_format` option allows the timestamps to be parsed using a custom datetime format.
It accepts the https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior[formatting options] supported by Python.

```toml
data_datetime_format = "%Y/%m/%dT%H:%M"
```

=== Custom timezone

The `data_timezone` option allows to specify the time zone of the timestamps.
This option should only be used when the timestamps in the source do not contain time zone information.

```toml
data_timezone = "America/Sao_Paulo"
```

=== Row format

```toml
format = "row"
```

The row based format expects to find a Delta Lake at the given `uri` with at least 3 columns:

- The first column contains the `series name` as a string
- The second column contains the timestamp
- The third column contains the value as a numerical type or as strings
- A fourth optional column contains quality data

Guidance and mapping options for the quality column can be found in the
ifdef::sources[]
<<Quality, source documentation>>
endif::sources[]
ifndef::sources[]
link:sources.asciidoc#Quality[source documentation]
endif::sources[]
.

Alternatively,
if the `tag_columns` and `field_columns` options are used,
each combination of values defined in `tag_columns` and `field_columns` define a series.

=== Pivot Format

```toml
format = "pivot"
```

The pivot format expect to find a Delta Lake at the given `URI`.

The first column in the lake is a timestamp.
Further columns contain the values.
Some columns can be numerical while other columns contain strings.
The name of each column is the series name.
