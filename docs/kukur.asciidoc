= Kukur
:icons: font
:toc:
:kukur-documentation:
:sources:
:include-path: ./includes

// SPDX-FileCopyrightText: 2021 Timeseer.AI
//
// SPDX-License-Identifier: Apache-2.0

== What is Kukur?

Kukur makes time series data and metadata available to the https://arrow.apache.org/[Apache Arrow] ecosystem.
Kukur can be used as a Python library or as a standalone application that exposes an Arrow Flight interface.

[WARNING]
====
Kukur is under active development.
Breaking changes to the interfaces are possible.

Kukur uses semantic versioning.
While `< 1.0.0`, changes to the minor version indicate breaking changes.
====

Potential usage scenarios are:

- ad-hoc in a data project
- as a time series data integration hub on your own system
- as a centrally managed time series data integration hub
- as a library in a Python application that needs time series data

== Getting Started

This example shows how to:

- run Kukur in a data project,
- expose a CSV file through it
- and connect to it using the Kukur client.

The only prerequisite is a working Python 3 installation.
Minor changes to the shell commands, but not the Kukur configuration, are required depending on your OS.

=== Installation

First create a new directory and enter it:

[source,bash]
----
$ mkdir data-project
$ cd data-project
----

Create a Python virtualenv and activate it:

[source,bash]
----
$ python -m venv venv
$ source venv/bin/activate
----

Install Kukur and list the command line options to verify that the installation was OK:

[source,bash]
----
(venv) $ pip install kukur
(venv) $ kukur --help
usage: kukur [-h] [--config-file CONFIG_FILE] {flight,test,api-key} ...

Start Kukur.

positional arguments:
  {flight,test,api-key}
                        Select the CLI action
    flight              Enable the Arrow Flight interface (the default)
    test                Test data source connectivity
    api-key             Create an api key for the Arrow Flight interface

optional arguments:
  -h, --help            show this help message and exit
  --config-file CONFIG_FILE
                        Path to the configuration file
----

Kukur supports "extras" at install time.
Use these to install requirements for the given source.

- `[adodb]` for connections to ADODB and OLEDB data sources
- `[cratedb]` for connections to CrateDB
- `[delta]` for connections to Delta Lakes
- `[influxdb]` for connections to InfluxDB
- `[odbc]` for connections to ODBC data sources
- `[piwebapi]` for connections to PI Web API

For example:

[source,bash]
----
(venv) $ pip install kukur[adodb,odbc]
----

=== Configuration

Kukur connects to many different local or remote time series sources.
A local CSV file will be used in this example.

Create a directory `data/`:

[source,bash]
----
(venv) $ mkdir data
----

Add the CSV data in `data/example.csv`:

data/example.csv
----
outside-temperature,2020-01-02T00:00:00Z,1
outside-temperature,2020-01-02T01:00:00Z,2
outside-temperature,2020-01-02T02:00:00Z,3
----

The next step is to configure a Kukur data source that exposes this CSV to any client.

Kukur uses https://toml.io/[TOML] as its configuration language.
Create `Kukur.toml` in the root folder of your project.
Define a source called 'example' that exposes the `data/example.csv` CSV:

Kukur.toml
[source,toml]
----
[source.example]
type = "csv"
path = "data/example.csv"
----

Use the Kukur CLI to test connectivity:

[source,bash]
----
(venv) $ kukur test data \
    --source example \
    --name outside-temperature \
    --start 2020-01-01 \
    --end 2021-01-01
2021-03-29 11:12:37,855 INFO kukur.source.test MainThread : Requesting data for "outside-temperature (example)" from 2020-01-01 00:00:00 to 2021-01-01 00:00:00
2020-01-02T00:00:00+00:00,1
2020-01-02T01:00:00+00:00,2
2020-01-02T02:00:00+00:00,3
----

[TIP]
====
The Kukur CLI logs to stderr, while the data itself is printed to stdout.
The `test` CLI command can thus be (ab-)used to extract data from any configured data source as CSV.
====

Now, having this data is useful, but where Kukur sets itself apart is that it also provides an opinionated interface for metadata.

Is the outside temperature defined in Kelvin?
Unless we're probing a spacecraft at the dark side of the moon, this is unlikely, but there is no way to know.

Our thermometer probably has physical measurement limits as well.
When values outside the -20 deg C to 60 deg C scale that this particular thermometer supports appear,
using them is probably not a good idea.

Similarly, the person that was writing down the measurements is not able to read the values with infinite accuracy.
At best, there will be a 0.5 deg C accuracy margin for any measurement.

Many time series sources expose this kind of metadata and Kukur can read it.

Let's create another CSV file:

data/example-metadata.csv
----
series name,description,unit,physical lower limit,physical upper limit,accuracy
outside-temperature,Temperature in Antwerp,deg C,-20,60,0.5
----

Kukur can mix-and-match metadata.
For example,
data can be stored in an InfluxDB database,
while descriptions of the measurements are stored in a CSV file,
but the sensor limits are stored in MS SQL database.

Update the configuration:

Kukur.toml
[source,toml]
----
[source.example]
type = "csv"
path = "data/example.csv"
metadata = "data/example-metadata.csv"
----

Request the metadata using the CLI:

[source,bash]
----
(venv) $ kukur test metadata \
    --source example \
    --name outside-temperature
2021-03-29 11:41:48,936 INFO kukur.source.test MainThread : Requesting metadata for "outside-temperature (example)"
series name,description,unit,physical lower limit,physical upper limit,functional lower limit,functional upper limit,accuracy,interpolation type,data type,dictionary name,dictionary
outside-temperature,Temperature in Antwerp,deg C,-20.0,60.0,,,0.5,,,,
----

Many fields are blank because our CSV file did not contain them.

[TIP]
====
The interpolation type for example is a very important piece of metadata.
When resampling values of multiple time series to the same timestamps,
using linear interpolation most likely results in different values than using stepwise interpolation.
====

=== Using the Kukur Client

Now, having validated the Kukur configuration, let's start the Kukur server:

[source,bash]
----
(venv) $ kukur
----

Open another shell, enter the virtualenv, start Python and import all Kukur objects:

[source,bash]
----
$ source venv/bin/activate
(venv) $ python
Python 3.9.2 (default, Feb 20 2021, 18:40:11)
[GCC 10.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from kukur import *
----

Let's try to request the metadata:

[source,python]
----
>>> client = Client()
>>> client.get_metadata(SeriesSelector('example', 'outside-temperature'))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "data-project/venv/lib/python3.9/site-packages/kukur/client.py", line 73, in get_metadata
    results = list(
  File "pyarrow/_flight.pyx", line 1239, in do_action
  File "pyarrow/_flight.pyx", line 66, in pyarrow._flight.check_flight_status
pyarrow._flight.FlightUnauthenticatedError: gRPC returned unauthenticated error, with message: invalid token. Detail: Unauthenticated
----

Kukur is secure by default and does not allow unauthenticated access.
Since we're running Kukur locally, it's OK to enable anonymous access.

First stop Kukur.
Then add a `[flight]` section to the configuration in `Kukur.toml`:

Kukur.toml
[source,toml]
----
[flight]
authentication = false

[source.example]
type = "csv"
path = "data/example.csv"
metadata = "data/example-metadata.csv"
----

Restart Kukur:

[source,bash]
----
(venv) $ kukur
----

Alternatively, use `kukur api-key` to define local API keys.

Now, go back to the Python session and request the metadata:

[source,python]
----
>>> client.get_metadata(SeriesSelector('example', 'outside-temperature'))
Metadata(SeriesSelector(source='example', name='outside-temperature'), {'description': 'Temperature in Antwerp', 'unit': 'deg C', 'physical lower limit': -20.0, 'physical upper limit': 60.0, 'functional lower limit': None, 'functional upper limit': None, 'accuracy': 0.5, 'interpolation type': None, 'data type': None, 'dictionary name': None, 'dictionary': None})
----

Finally, read the data:

[source,python]
----
>>> from datetime import datetime
>>> client.get_data(
        SeriesSelector('example', 'outside-temperature'),
        datetime.fromisoformat('2020-01-01T00:00:00+00:00'),
        datetime.fromisoformat('2021-01-01T00:00:00+00:00'),
    )
pyarrow.Table
ts: timestamp[us, tz=utc]
value: int64
----

Data is always returned as an Apache Arrow table with two columns: a timestamp and a value.

[source,python]
----
>>> table = _
>>> table.to_pydict()
{'ts': [datetime.datetime(2020, 1, 2, 0, 0, tzinfo=<UTC>), datetime.datetime(2020, 1, 2, 1, 0, tzinfo=<UTC>), datetime.datetime(2020, 1, 2, 2, 0, tzinfo=<UTC>)], 'value': [1, 2, 3]}
----

Using Kukur, we now have metadata and data in a format that allows us to correctly analyze our outside temperature.

More importantly: we have made data access scalable,
as the Kukur configuration can be used the next time data is needed.
To do so we can store the Kukur configuration in a version control system, such as `git`.

=== Storing the Configuration in Version Control

This requires `git` to be installed.

Create a local repository by running:

[source,bash]
----
$ git init .
----

Ignore the data, virtualenv and Kukur databases using a `.gitignore` file:

+.gitignore+
----
data/
venv/
*.sqlite
----

Now track the current revision of the Kukur configuration:

[source,bash]
----
$ git add Kukur.toml
$ git commit -v
----

This repository can now be shared with other people,
effortlessly granting them access to the same data sources.

=== Using Docker

In a data project that does not use Python,
the https://hub.docker.com/r/timeseer/kukur[Kukur Docker container] can be used to export data as CSV for any supported source.

[source,bash]
----
$ docker run --rm \
    -u $(id -u) \
    -v $(pwd)/Kukur.toml:/usr/src/app/Kukur.toml \
    -v $(pwd)/data:/usr/src/app/data \
    -it timeseer/kukur:latest python -m kukur.cli test data \
        --source example \
        --name outside-temperature \
        --start 2020-01-01 \
        --end 2021-01-01
----

`-u $(id -u)`::
Run Kukur using your user.
This ensures that permissions for volume mounts are correct,
since Kukur does not run as root inside the container.
`-v $(pwd)/Kukur.toml:/usr/src/app/Kukur.toml`::
This mounts the Kukur configuration file `Kukur.toml` to the expected location inside the container.
By using the `--config-file` command line flag, a different location can be chosen.
`-v $(pwd)/data:/usr/src/app/data`::
This mounts the CSV example data on the location that is specified in the configuration.
Not required when connecting to data sources that are not file-based.

The Arrow Flight interface is made available on port 8081 when running Kukur as a service.
Use port mapping to expose it on localhost (`-p 8081:8081`).

For API key storage,
Kukur creates a sqlite database and stores it in the path given by `data_dir`.
Define a data directory in `Kukur.toml`:

Kukur.toml
[source,toml]
----
data_dir = "db"

[flight]
authentication = false

[source.example]
type = "csv"
path = "data/example.csv"
metadata = "data/example-metadata.csv"
----

Create that directory and run the docker container while mounting it:

[source,bash]
----
$ mkdir db
$ docker run --rm \
    -u $(id -u) \
    -v $(pwd)/Kukur.toml:/usr/src/app/Kukur.toml \
    -v $(pwd)/data:/usr/src/app/data \
    -v $(pwd)/db:/usr/src/app/db \
    -p 8081:8081 \
    -it timeseer/kukur:latest
----

Then, access it using the Kukur client:

[source,bash]
----
(venv) $ python
Python 3.9.2 (default, Feb 20 2021, 18:40:11)
[GCC 10.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from kukur import *
>>> client = Client()
>>> client.get_metadata(SeriesSelector('example', 'outside-temperature'))
Metadata(SeriesSelector(source='example', name='outside-temperature'), {'description': 'Temperature in Antwerp', 'unit': 'deg C', 'physical lower limit': -20.0, 'physical upper limit': 60.0, 'functional lower limit': None, 'functional upper limit': None, 'accuracy': 0.5, 'interpolation type': None, 'data type': None, 'dictionary name': None, 'dictionary': None})
----

== Development

Kukur is developed on GitHub.
Visit https://github.com/timeseer-ai/kukur to learn more.

Kukur is open source software, licensed under the Apache License, Version 2.0.

=== Domain Model

To understand Kukur,
three concepts need to be understood:

- <<SeriesSelector>>
- <<Metadata>>
- <<Source>>

==== SeriesSelector

[plantuml]
....
class SeriesSelector {
    source : str
    tags : Union[str, dict[str, str]]
    field: str = "value"
}
....

A time series data source contains hundreds, thousands or even millions of time series.
A convenient and expressive way of selecting which time series should be queried is required.
This is a `SeriesSelector`.

The simplest mapping is associating a unique name with each series.
It follows that the minimum unique identifier includes the source and the name of the time series.

[cols="3*"]
|===
| Timestamp
|Series name
| Value
|===

Many time series databases add additional structure to this.

In InfluxDB, for example, the layout looks like:

[cols="6*"]
|===
| Timestamp
3+| Series
2+| Values

| Timestamp
| Measurement
| Tag A
| Tag B
| Field A
| Field B

|===

A time series is identified by a 'measurement name', multiple 'tag key-value pairs' and a 'field key'.

NOTE: Kukur will move from alpha to beta status once support for this structure has been implemented.

==== Metadata

Kukur predefines a set of metadata fields that are all present in an ideal world.
Custom fields can be added both programmatically and using configuration.

[plantuml]
....
class Metadata {
    series: SeriesSelector
    description : str
    unit : str
    limit_low_physical : Optional[float]
    limit_high_physical : Optional[float]
    limit_low_functional : Optional[float]
    limit_high_functional : Optional[float]
    accuracy : Optional[float]
    interpolation_type: Optional[InterpolationType]
    data_type: Optional[DataType]
    dictionary_name: Optional[str]
    dictionary: Optional[Dictionary]
}

enum InterpolationType {
    LINEAR
    STEPPED
}

enum DataType {
    FLOAT32
    FLOAT64
    STRING
    DICTIONARY
}

class Dictionary {
    mapping: Dict[int, str]
}
....

Both physical and functional limits are configurable.
Physical limits are limits of the actual sensor that collected the time series data.
Functional limits are the limits of the expected range of the time series.
For example, while my thermometer could indicate -20 deg C (physical lower limit),
anything below -5 deg C (functional lower limit) would be cause for alarm.

The `DICTIONARY` `DataType` warrants an explanation as well.

Time series of type `DICTIONARY` store numerical values.
Each numerical value is given a meaning.

For example: the state of a pump could be `ON` or `OFF`.
`OFF` can be represented as `0` in the time series data, while `ON` could be encoded as `1`.

These dictionaries that map numerical values to string labels are often named as well,
hence the `dictionary name` field.

==== Source

Kukur data sources implement at least three methods:

[plantuml]
....
interface Source {
    search()
    get_metadata()
    get_data()
}
....

`search(SeriesSelector) -> Generator[Union[SeriesSelector, Metadata]]`::
Return all time series matching the selector or even the metadata of them if it is readily available.
`get_metadata(SeriesSelector) -> Metadata`::
Return metadata for the selected time series.
`get_data(SeriesSelector, Datetime, Datetime) -> pyarrow.Table`::
Return data for the selected time series in the given time period.

== Configuration Reference

Kukur is configured by editing a configuration file in the https://toml.io/[TOML] language.

The default location of this configuration file is `Kukur.toml`.
Alternative paths can be configured using the `--config-file` flag.

=== Data Storage

Kukur needs local data storage for API keys.

The path to the local data storage of Kukur is configured by:

[source,toml]
----
data_dir = "<path to data directory>"
----

`data_dir` is not required.
By default, data is stored in the current working directory.

=== Includes

Other configuration files can be included from the main configuration file by using `[[include]]`.
Multiple includes can be specified by using an array of tables:

[source,toml]
----
[[include]]
glob = "<file glob>"
----

The `glob` key is required.

Paths in includes files are resolved relative to the application working directory.
They are not relative to the included configuration file.

Conflicts between configuration values are handled depending on the value type:

- `string`: the last included field overrides earlier values
- `list`: the items in the list are appended to the earlier list
- `mapping`: the earlier mapping is updated with the new mapping, overriding existing keys

Note that the main configuration file is processed before any includes.
This means it is not possible to override configuration set by an include from the main configuration file.

Example:

[source,toml]
----
[[include]]
glob = "tests/test_data/*/*.toml"
----

This will include all TOML files in a direct subdirectory of `tests/test_data/`.

For example, `tests/test_data/csv/csv-examples.toml` could contain:

[source,toml]
----
[source.row]
type = "csv"
path = "examples/csv/row.csv"
metadata = "examples/csv/row-metadata.csv"
----

=== Arrow Flight

A `[flight]` section configures the Arrow Flight interface.

[source,toml]
----
[flight]
host = "0.0.0.0"
port = 8081
authentication = true
----

Kukur listens on port `8081` of all IP addresses by default.

Authentication can be turned off for local instances or when provided by external services
by setting `authentication` to `false`.
When authentication is turned on, an API key has to be supplied by callers.

== Sources

include::./source/sources.asciidoc[leveloffset=+1]

== Sources reference

include::./source/adodb.asciidoc[leveloffset=+1]

include::./source/feather.asciidoc[leveloffset=+1]

include::./source/parquet.asciidoc[leveloffset=+1]

include::./source/cratedb.asciidoc[leveloffset=+1]

include::./source/csv.asciidoc[leveloffset=+1]

include::./source/data_explorer.asciidoc[leveloffset=+1]

include::./source/delta.asciidoc[leveloffset=+1]

include::./source/influxdb.asciidoc[leveloffset=+1]

include::./source/kukur.asciidoc[leveloffset=+1]

include::./source/odbc.asciidoc[leveloffset=+1]

include::./source/piwebapi-da.asciidoc[leveloffset=+1]

== Connectivity guides

Kukur can connect to many different time series data sources using the options detailed in the <<Sources reference>>.
Concrete examples of complex integrations are documented here.

include::./ip21.asciidoc[leveloffset=+2]
include::./azure-synapse.asciidoc[leveloffset=+2]
include::./proficy.asciidoc[leveloffset=+2]
include::./pi.asciidoc[leveloffset=+2]
